{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE ==============================================\n",
    "#\n",
    "# Fill in code for every method which has a TODO\n",
    "#\n",
    "# Your implementation should use the contract (inputs\n",
    "# and outputs) given for each model, because that is \n",
    "# what the main script expects. If you modify the contract, \n",
    "# you must justify that choice, note it in your report, and notify the TAs \n",
    "# so that we run the correct code.\n",
    "#\n",
    "# You may modify the internals of the RNN and GRU classes\n",
    "# as much as you like, except you must keep the methods\n",
    "# in each (init_weights_uniform, init_hidden, and forward)\n",
    "# Using nn.Module and \"forward\" tells torch which \n",
    "# parameters are involved in the forward pass, so that it\n",
    "# can correctly (automatically) set up the backward pass.\n",
    "#\n",
    "# You should not modify the interals of the Transformer\n",
    "# except where indicated to implement the multi-head\n",
    "# attention. \n",
    "\n",
    "\n",
    "def clones(module, N):\n",
    "    \"A helper function for producing N identical layers (each with their own parameters).\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "# Problem 1\n",
    "class RNN(nn.Module): # Implement a stacked vanilla RNN with Tanh nonlinearities.\n",
    "  def __init__(self, emb_size, hidden_size, seq_len, batch_size, vocab_size, num_layers, dp_keep_prob):\n",
    "    # TODO ========================\n",
    "    # Initialization of the parameters of the recurrent and fc layers. \n",
    "    # Your implementation should support any number of stacked hidden layers \n",
    "    # (specified by num_layers), use an input embedding layer, and include fully\n",
    "    # connected layers with dropout after each recurrent layer.\n",
    "    # Note: you may use pytorch's nn.Linear, nn.Dropout, and nn.Embedding \n",
    "    # modules, but not recurrent modules.\n",
    "    #\n",
    "    # To create a variable number of parameter tensors and/or nn.Modules \n",
    "    # (for the stacked hidden layer), you may need to use nn.ModuleList or the \n",
    "    # provided clones function (as opposed to a regular python list), in order \n",
    "    # for Pytorch to recognize these parameters as belonging to this nn.Module \n",
    "    # and compute their gradients automatically. You're not obligated to use the\n",
    "    # provided clones function.\n",
    "    \"\"\"\n",
    "    emb_size:     The numvwe of units in the input embeddings\n",
    "    hidden_size:  The number of hidden units per layer\n",
    "    seq_len:      The length of the input sequences\n",
    "    vocab_size:   The number of tokens in the vocabulary (10,000 for Penn TreeBank)\n",
    "    num_layers:   The depth of the stack (i.e. the number of hidden layers at \n",
    "                  each time-step)\n",
    "    dp_keep_prob: The probability of *not* dropping out units in the \n",
    "                  non-recurrent connections.\n",
    "                  Do not apply dropout on recurrent connections.\n",
    "    \"\"\"\n",
    "    super(RNN, self).__init__()\n",
    "    \n",
    "    # Parameters\n",
    "    self.emb_size = emb_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.seq_len = seq_len\n",
    "    self.batch_size = batch_size\n",
    "    self.vocab_size = vocab_size\n",
    "    self.num_layers = num_layers\n",
    "    self.dp_keep_prob = dp_keep_prob\n",
    "    \n",
    "    # RNN Parameters :\n",
    "    W_x = nn.ParameterList([nn.Parameter(torch.Tensor(hidden_size, emb_size))])\n",
    "    W_x.extend([nn.Parameter(torch.Tensor(hidden_size, hidden_size)) for _ in range(num_layers-1)])\n",
    "    W_h = nn.ParameterList([nn.Parameter(torch.Tensor(hidden_size, hidden_size)) for _ in range(num_layers)])\n",
    "    W_y = nn.Parameter(torch.Tensor(vocab_size, hidden_size))\n",
    "    \n",
    "    b_h = nn.ParameterList([nn.Parameter(torch.Tensor(hidden_size)) for _ in range(num_layers)])\n",
    "    b_y = nn.Parameter(torch.Tensor(vocab_size))\n",
    "    \n",
    "    self.weights = [W_x, W_h, W_y]\n",
    "    self.bias = [b_h, b_y]\n",
    "    \n",
    "    # Hidden layers : \n",
    "    self.hidden_layers = torch.Tensor(self.num_layers, self.batch_size, self.hidden_size)\n",
    "    \n",
    "    # Word Embedding Layer : \n",
    "    self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "    \n",
    "    # FC Layers and Dropout Layers :\n",
    "    self.linear_layers = clones(nn.Linear(hidden_size, hidden_size), num_layers-1)\n",
    "    self.dropout_layers = clones(nn.Dropout(p = 1-dp_keep_prob), num_layers-1)\n",
    "    self.linear_layers.append(nn.Linear(vocab_size, hidden_size))\n",
    "    \n",
    "\n",
    "  def init_weights_uniform(self):\n",
    "    # TODO ========================\n",
    "    # Initialize all the weights uniformly in the range [-0.1, 0.1]\n",
    "    # and all the biases to 0 (in place)\n",
    "    for i in range(self.num_layer):\n",
    "        nn.init.uniform_(self.weights[0][i], -0.1, 0.1)\n",
    "        nn.init.uniform_(self.weights[1][i], -0.1, 0.1)\n",
    "        nn.init.constant_(self.bias[0][i], 0)\n",
    "        \n",
    "    nn.init.uniform_(self.weights[2], -0.1, 0.1)\n",
    "    nn.init.constant_(self.bias[1], 0)\n",
    "    \n",
    "    \n",
    "\n",
    "  def init_hidden(self):\n",
    "    # TODO ========================\n",
    "    # initialize the hidden states to zero\n",
    "    \"\"\"\n",
    "    This is used for the first mini-batch in an epoch, only.\n",
    "    \"\"\"\n",
    "    nn.init.constant_(self.hidden_layers, 0)\n",
    "    return self.hidden_layers\n",
    "    # a parameter tensor of shape (self.num_layers, self.batch_size, self.hidden_size)\n",
    "\n",
    "  def forward(self, inputs, hidden):\n",
    "    # TODO ========================\n",
    "    # Compute the forward pass, using a nested python for loops.\n",
    "    # The outer for loop should iterate over timesteps, and the \n",
    "    # inner for loop should iterate over hidden layers of the stack. \n",
    "    # \n",
    "    # Within these for loops, use the parameter tensors and/or nn.modules you \n",
    "    # created in __init__ to compute the recurrent updates according to the \n",
    "    # equations provided in the .tex of the assignment.\n",
    "    #\n",
    "    # Note that those equations are for a single hidden-layer RNN, not a stacked\n",
    "    # RNN. For a stacked RNN, the hidden states of the l-th layer are used as \n",
    "    # inputs to to the {l+1}-st layer (taking the place of the input sequence).\n",
    "\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        - inputs: A mini-batch of input sequences, composed of integers that \n",
    "                    represent the index of the current token(s) in the vocabulary.\n",
    "                        shape: (seq_len, batch_size)\n",
    "        - hidden: The initial hidden states for every layer of the stacked RNN.\n",
    "                        shape: (num_layers, batch_size, hidden_size)\n",
    "    \n",
    "    Returns:\n",
    "        - Logits for the softmax over output tokens at every time-step.\n",
    "              **Do NOT apply softmax to the outputs!**\n",
    "              Pytorch's CrossEntropyLoss function (applied in ptb-lm.py) does \n",
    "              this computation implicitly.\n",
    "                    shape: (seq_len, batch_size, vocab_size)\n",
    "        - The final hidden states for every layer of the stacked RNN.\n",
    "              These will be used as the initial hidden states for all the \n",
    "              mini-batches in an epoch, except for the first, where the return \n",
    "              value of self.init_hidden will be used.\n",
    "              See the repackage_hiddens function in ptb-lm.py for more details, \n",
    "              if you are curious.\n",
    "                    shape: (num_layers, batch_size, hidden_size)\n",
    "    \"\"\"\n",
    "    return logits.view(self.seq_len, self.batch_size, self.vocab_size), hidden\n",
    "\n",
    "  def generate(self, input, hidden, generated_seq_len):\n",
    "    # TODO ========================\n",
    "    # Compute the forward pass, as in the self.forward method (above).\n",
    "    # You'll probably want to copy substantial portions of that code here.\n",
    "    # \n",
    "    # We \"seed\" the generation by providing the first inputs.\n",
    "    # Subsequent inputs are generated by sampling from the output distribution, \n",
    "    # as described in the tex (Problem 5.3)\n",
    "    # Unlike for self.forward, you WILL need to apply the softmax activation \n",
    "    # function here in order to compute the parameters of the categorical \n",
    "    # distributions to be sampled from at each time-step.\n",
    "\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        - input: A mini-batch of input tokens (NOT sequences!)\n",
    "                        shape: (batch_size)\n",
    "        - hidden: The initial hidden states for every layer of the stacked RNN.\n",
    "                        shape: (num_layers, batch_size, hidden_size)\n",
    "        - generated_seq_len: The length of the sequence to generate.\n",
    "                       Note that this can be different than the length used \n",
    "                       for training (self.seq_len)\n",
    "    Returns:\n",
    "        - Sampled sequences of tokens\n",
    "                    shape: (generated_seq_len, batch_size)\n",
    "    \"\"\"\n",
    "   \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 2\n",
    "class GRU(nn.Module): # Implement a stacked GRU RNN\n",
    "  \"\"\"\n",
    "  Follow the same instructions as for RNN (above), but use the equations for \n",
    "  GRU, not Vanilla RNN.\n",
    "  \"\"\"\n",
    "  def __init__(self, emb_size, hidden_size, seq_len, batch_size, vocab_size, num_layers, dp_keep_prob):\n",
    "    super(GRU, self).__init__()\n",
    "\n",
    "    # TODO ========================\n",
    "\n",
    "  def init_weights_uniform(self):\n",
    "    # TODO ========================\n",
    "\n",
    "  def init_hidden(self):\n",
    "    # TODO ========================\n",
    "    return # a parameter tensor of shape (self.num_layers, self.batch_size, self.hidden_size)\n",
    "\n",
    "  def forward(self, inputs, hidden):\n",
    "    # TODO ========================\n",
    "    return logits.view(self.seq_len, self.batch_size, self.vocab_size), hidden\n",
    "\n",
    "  def generate(self, input, hidden, generated_seq_len):\n",
    "    # TODO ========================\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 3\n",
    "##############################################################################\n",
    "#\n",
    "# Code for the Transformer model\n",
    "#\n",
    "##############################################################################\n",
    "\n",
    "\"\"\"\n",
    "Implement the MultiHeadedAttention module of the transformer architecture.\n",
    "All other necessary modules have already been implemented for you.\n",
    "\n",
    "We're building a transfomer architecture for next-step prediction tasks, and \n",
    "applying it to sequential language modelling. We use a binary \"mask\" to specify \n",
    "which time-steps the model can use for the current prediction.\n",
    "This ensures that the model only attends to previous time-steps.\n",
    "\n",
    "The model first encodes inputs using the concatenation of a learned WordEmbedding \n",
    "and a (in our case, hard-coded) PositionalEncoding.\n",
    "The word embedding maps a word's one-hot encoding into a dense real vector.\n",
    "The positional encoding 'tags' each element of an input sequence with a code that \n",
    "identifies it's position (i.e. time-step).\n",
    "\n",
    "These encodings of the inputs are then transformed repeatedly using multiple\n",
    "copies of a TransformerBlock.\n",
    "This block consists of an application of MultiHeadedAttention, followed by a \n",
    "standard MLP; the MLP applies *the same* mapping at every position.\n",
    "Both the attention and the MLP are applied with Resnet-style skip connections, \n",
    "and layer normalization.\n",
    "\n",
    "The complete model consists of the embeddings, the stacked transformer blocks, \n",
    "and a linear layer followed by a softmax.\n",
    "\"\"\"\n",
    "\n",
    "#This code has been modified from an open-source project, by David Krueger.\n",
    "#The original license is included below:\n",
    "#MIT License\n",
    "#\n",
    "#Copyright (c) 2018 Alexander Rush\n",
    "#\n",
    "#Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "#of this software and associated documentation files (the \"Software\"), to deal\n",
    "#in the Software without restriction, including without limitation the rights\n",
    "#to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "#copies of the Software, and to permit persons to whom the Software is\n",
    "#furnished to do so, subject to the following conditions:\n",
    "#\n",
    "#The above copyright notice and this permission notice shall be included in all\n",
    "#copies or substantial portions of the Software.\n",
    "#\n",
    "#THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "#IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "#FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "#AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "#LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "#OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "#SOFTWARE.\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------\n",
    "\n",
    "# TODO: implement this class\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, n_heads, n_units, dropout=0.1):\n",
    "        \"\"\"\n",
    "        n_heads: the number of attention heads\n",
    "        n_units: the number of output units\n",
    "        dropout: probability of DROPPING units\n",
    "        \"\"\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        # This sets the size of the keys, values, and queries (self.d_k) to all \n",
    "        # be equal to the number of output units divided by the number of heads.\n",
    "        self.d_k = n_units // n_heads\n",
    "        # This requires the number of n_heads to evenly divide n_units.\n",
    "        assert n_units % n_heads == 0\n",
    "        self.n_units = n_units \n",
    "\n",
    "        # TODO: create/initialize any necessary parameters or layers\n",
    "        # Note: the only Pytorch modules you are allowed to use are nn.Linear \n",
    "        # and nn.Dropout\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # TODO: implement the masked multi-head attention.\n",
    "        # query, key, and value all have size: (batch_size, seq_len, self.n_units, self.d_k)\n",
    "        # mask has size: (batch_size, seq_len, seq_len)\n",
    "        # As described in the .tex, apply input masking to the softmax \n",
    "        # generating the \"attention values\" (i.e. A_i in the .tex)\n",
    "        # Also apply dropout to the attention values.\n",
    "\n",
    "        return # size: (batch_size, seq_len, self.n_units)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------\n",
    "# The encodings of elements of the input sequence\n",
    "\n",
    "class WordEmbedding(nn.Module):\n",
    "    def __init__(self, n_units, vocab):\n",
    "        super(WordEmbedding, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, n_units)\n",
    "        self.n_units = n_units\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print (x)\n",
    "        return self.lut(x) * math.sqrt(self.n_units)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, n_units, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, n_units)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, n_units, 2).float() *\n",
    "                             -(math.log(10000.0) / n_units))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------\n",
    "# The TransformerBlock and the full Transformer\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(ResidualSkipConnectionWithLayerNorm(size, dropout), 2)\n",
    " \n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask)) # apply the self-attention\n",
    "        return self.sublayer[1](x, self.feed_forward) # apply the position-wise MLP\n",
    "\n",
    "\n",
    "class TransformerStack(nn.Module):\n",
    "    \"\"\"\n",
    "    This will be called on the TransformerBlock (above) to create a stack.\n",
    "    \"\"\"\n",
    "    def __init__(self, layer, n_blocks): # layer will be TransformerBlock (below)\n",
    "        super(TransformerStack, self).__init__()\n",
    "        self.layers = clones(layer, n_blocks)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "class FullTransformer(nn.Module):\n",
    "    def __init__(self, transformer_stack, embedding, n_units, vocab_size):\n",
    "        super(FullTransformer, self).__init__()\n",
    "        self.transformer_stack = transformer_stack\n",
    "        self.embedding = embedding\n",
    "        self.output_layer = nn.Linear(n_units, vocab_size)\n",
    "        \n",
    "    def forward(self, input_sequence, mask):\n",
    "        embeddings = self.embedding(input_sequence)\n",
    "        return F.log_softmax(self.output_layer(self.transformer_stack(embeddings, mask)), dim=-1)\n",
    "\n",
    "\n",
    "def make_model(vocab_size, n_blocks=6, \n",
    "               n_units=512, n_heads=16, dropout=0.1):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(n_heads, n_units)\n",
    "    ff = MLP(n_units, dropout)\n",
    "    position = PositionalEncoding(n_units, dropout)\n",
    "    model = FullTransformer(\n",
    "        transformer_stack=TransformerStack(TransformerBlock(n_units, c(attn), c(ff), dropout), n_blocks),\n",
    "        embedding=nn.Sequential(WordEmbedding(n_units, vocab_size), c(position)),\n",
    "        n_units=n_units,\n",
    "        vocab_size=vocab_size\n",
    "        )\n",
    "    \n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------\n",
    "# Data processing\n",
    "\n",
    "def subsequent_mask(size):\n",
    "    \"\"\" helper function for creating the masks. \"\"\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0\n",
    "\n",
    "class Batch:\n",
    "    \"Object for holding a batch of data with mask during training.\"\n",
    "    def __init__(self, x, pad=0):\n",
    "        self.data = x\n",
    "        self.mask = self.make_mask(self.data, pad)\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_mask(data, pad):\n",
    "        \"Create a mask to hide future words.\"\n",
    "        mask = (data != pad).unsqueeze(-2)\n",
    "        mask = mask & Variable(\n",
    "            subsequent_mask(data.size(-1)).type_as(mask.data))\n",
    "        return mask\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------\n",
    "# Some standard modules\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"layer normalization, as in: https://arxiv.org/abs/1607.06450\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "\n",
    "class ResidualSkipConnectionWithLayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(ResidualSkipConnectionWithLayerNorm, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    This is just an MLP with 1 hidden layer\n",
    "    \"\"\"\n",
    "    def __init__(self, n_units, dropout=0.1):\n",
    "        super(MLP, self).__init__()\n",
    "        self.w_1 = nn.Linear(n_units, 2048)\n",
    "        self.w_2 = nn.Linear(2048, n_units)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
