{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE ==============================================\n",
    "#\n",
    "# Fill in code for every method which has a TODO\n",
    "#\n",
    "# Your implementation should use the contract (inputs\n",
    "# and outputs) given for each model, because that is \n",
    "# what the main script expects. If you modify the contract, \n",
    "# you must justify that choice, note it in your report, and notify the TAs \n",
    "# so that we run the correct code.\n",
    "#\n",
    "# You may modify the internals of the RNN and GRU classes\n",
    "# as much as you like, except you must keep the methods\n",
    "# in each (init_weights_uniform, init_hidden, and forward)\n",
    "# Using nn.Module and \"forward\" tells torch which \n",
    "# parameters are involved in the forward pass, so that it\n",
    "# can correctly (automatically) set up the backward pass.\n",
    "#\n",
    "# You should not modify the interals of the Transformer\n",
    "# except where indicated to implement the multi-head\n",
    "# attention. \n",
    "\n",
    "\n",
    "def clones(module, N):\n",
    "    \"A helper function for producing N identical layers (each with their own parameters).\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "# Problem 1\n",
    "class RNN(nn.Module): # Implement a stacked vanilla RNN with Tanh nonlinearities.\n",
    "  def __init__(self, emb_size, hidden_size, seq_len, batch_size, vocab_size, num_layers, dp_keep_prob):\n",
    "    # TODO ========================\n",
    "    # Initialization of the parameters of the recurrent and fc layers. \n",
    "    # Your implementation should support any number of stacked hidden layers \n",
    "    # (specified by num_layers), use an input embedding layer, and include fully\n",
    "    # connected layers with dropout after each recurrent layer.\n",
    "    # Note: you may use pytorch's nn.Linear, nn.Dropout, and nn.Embedding \n",
    "    # modules, but not recurrent modules.\n",
    "    #\n",
    "    # To create a variable number of parameter tensors and/or nn.Modules \n",
    "    # (for the stacked hidden layer), you may need to use nn.ModuleList or the \n",
    "    # provided clones function (as opposed to a regular python list), in order \n",
    "    # for Pytorch to recognize these parameters as belonging to this nn.Module \n",
    "    # and compute their gradients automatically. You're not obligated to use the\n",
    "    # provided clones function.\n",
    "    \"\"\"\n",
    "    emb_size:     The numvwe of units in the input embeddings\n",
    "    hidden_size:  The number of hidden units per layer\n",
    "    seq_len:      The length of the input sequences\n",
    "    vocab_size:   The number of tokens in the vocabulary (10,000 for Penn TreeBank)\n",
    "    num_layers:   The depth of the stack (i.e. the number of hidden layers at \n",
    "                  each time-step)\n",
    "    dp_keep_prob: The probability of *not* dropping out units in the \n",
    "                  non-recurrent connections.\n",
    "                  Do not apply dropout on recurrent connections.\n",
    "    \"\"\"\n",
    "    super(RNN, self).__init__()\n",
    "    \n",
    "    # Parameters\n",
    "    self.emb_size = emb_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.seq_len = seq_len\n",
    "    self.batch_size = batch_size\n",
    "    self.vocab_size = vocab_size\n",
    "    self.num_layers = num_layers\n",
    "    self.dp_keep_prob = dp_keep_prob\n",
    "    \n",
    "#     # RNN Parameters :\n",
    "#     W_x = nn.ParameterList([nn.Parameter(torch.Tensor(hidden_size, emb_size))])\n",
    "#     W_x.extend([nn.Parameter(torch.Tensor(hidden_size, hidden_size)) for _ in range(num_layers-1)])\n",
    "#     W_h = nn.ParameterList([nn.Parameter(torch.Tensor(hidden_size, hidden_size)) for _ in range(num_layers)])\n",
    "#     W_y = nn.Parameter(torch.Tensor(vocab_size, hidden_size))\n",
    "    \n",
    "#     b_h = nn.ParameterList([nn.Parameter(torch.Tensor(hidden_size)) for _ in range(num_layers)])\n",
    "#     b_y = nn.Parameter(torch.Tensor(vocab_size))\n",
    "    \n",
    "#     self.weights = [W_x, W_h, W_y]\n",
    "#     self.bias = [b_h, b_y]\n",
    "\n",
    "    # RNN Layer : \n",
    "    first_rnn = nn.Linear(self.emb_size + self.hidden_size, self.hidden_size)\n",
    "    self.recurrent_layers = nn.ModuleList([first_rnn])\n",
    "    other_rnn = nn.Linear(2*self.hidden_size, self.hidden_size)\n",
    "    self.recurrent_layers.extend(clones(other_rnn, self.num_layers-1))\n",
    "        \n",
    "    # Word Embedding Layer : \n",
    "    self.embedding = nn.Embedding(num_embeddings = vocab_size, embedding_dim = emb_size)\n",
    "    \n",
    "    # FC Layers and Dropout Layers :\n",
    "    self.linear_layers = clones(nn.Linear(hidden_size, hidden_size), num_layers-1)\n",
    "    self.linear_layers.append(nn.Linear(hidden_size, vocab_size))\n",
    "    \n",
    "    self.dropout_layers = clones(nn.Dropout(p = 1-dp_keep_prob), num_layers)\n",
    "    \n",
    "    # Hidden layers coefficient: \n",
    "    #self.hidden_layers = torch.Tensor(self.num_layers, self.batch_size, self.hidden_size)\n",
    "\n",
    "    \n",
    "\n",
    "  def init_weights_uniform(self):\n",
    "    # TODO ========================\n",
    "    # Initialize all the weights uniformly in the range [-0.1, 0.1]\n",
    "    # and all the biases to 0 (in place)\n",
    "    for name, param in self.recurrent_layers.named_parameters():\n",
    "        if name == 'weight':\n",
    "            nn.init.uniform(param, -0.1, 0.1)\n",
    "        else:\n",
    "            nn.init.constant(param, 0)\n",
    "    \n",
    "    for name, param in self.linear_layers.named_parameters():\n",
    "        if name == 'weight':\n",
    "            nn.init.uniform(param, -0.1, 0.1)\n",
    "        else:\n",
    "            nn.init.constant(param, 0)\n",
    "#     for i in range(self.num_layers):\n",
    "#         nn.init.uniform_(self.weights[0][i], -0.1, 0.1)\n",
    "#         nn.init.uniform_(self.weights[1][i], -0.1, 0.1)\n",
    "#         nn.init.constant_(self.bias[0][i], 0)\n",
    "        \n",
    "#     nn.init.uniform_(self.weights[2], -0.1, 0.1)\n",
    "#     nn.init.constant_(self.bias[1], 0)\n",
    "    \n",
    "    \n",
    "\n",
    "  def init_hidden(self):\n",
    "    # TODO ========================\n",
    "    # initialize the hidden states to zero\n",
    "    \"\"\"\n",
    "    This is used for the first mini-batch in an epoch, only.\n",
    "    \"\"\"\n",
    "    #nn.init.constant_(self.hidden_layers, 0)\n",
    "    return nn.init.constant_(torch.Tensor(self.num_layers, self.batch_size, self.hidden_size), 0)\n",
    "    # a parameter tensor of shape (self.num_layers, self.batch_size, self.hidden_size)\n",
    "\n",
    "  def forward(self, inputs, hidden):\n",
    "    # TODO ========================\n",
    "    # Compute the forward pass, using a nested python for loops.\n",
    "    # The outer for loop should iterate over timesteps, and the \n",
    "    # inner for loop should iterate over hidden layers of the stack. \n",
    "    # \n",
    "    # Within these for loops, use the parameter tensors and/or nn.modules you \n",
    "    # created in __init__ to compute the recurrent updates according to the \n",
    "    # equations provided in the .tex of the assignment.\n",
    "    #\n",
    "    # Note that those equations are for a single hidden-layer RNN, not a stacked\n",
    "    # RNN. For a stacked RNN, the hidden states of the l-th layer are used as \n",
    "    # inputs to to the {l+1}-st layer (taking the place of the input sequence).\n",
    "\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        - inputs: A mini-batch of input sequences, composed of integers that \n",
    "                    represent the index of the current token(s) in the vocabulary.\n",
    "                        shape: (seq_len, batch_size)\n",
    "        - hidden: The initial hidden states for every layer of the stacked RNN.\n",
    "                        shape: (num_layers, batch_size, hidden_size)\n",
    "    \n",
    "    Returns:\n",
    "        - Logits for the softmax over output tokens at every time-step.\n",
    "              **Do NOT apply softmax to the outputs!**\n",
    "              Pytorch's CrossEntropyLoss function (applied in ptb-lm.py) does \n",
    "              this computation implicitly.\n",
    "                    shape: (seq_len, batch_size, vocab_size)\n",
    "        - The final hidden states for every layer of the stacked RNN.\n",
    "              These will be used as the initial hidden states for all the \n",
    "              mini-batches in an epoch, except for the first, where the return \n",
    "              value of self.init_hidden will be used.\n",
    "              See the repackage_hiddens function in ptb-lm.py for more details, \n",
    "              if you are curious.\n",
    "                    shape: (num_layers, batch_size, hidden_size)\n",
    "    \"\"\"\n",
    "    if inputs.is_cuda:\n",
    "            device = inputs.get_device()\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "    \n",
    "    hidden = hidden.to(device)\n",
    "    \n",
    "    emb_inputs = self.embedding(inputs)\n",
    "    logits = torch.Tensor(self.seq_len, self.batch_size, self.vocab_size).to(device)\n",
    "    \n",
    "    for seq in range(self.seq_len):\n",
    "        \n",
    "        hidden[0] = self.recurrent_layers[0](torch.cat([hidden[0],emb_inputs[seq]], 1))\n",
    "        outputs = self.dropout_layers[0](hidden[0])\n",
    "        outputs = self.linear_layers[0](outputs)\n",
    "        if self.num_layers == 1:\n",
    "            logits[seq] = outputs\n",
    "        \n",
    "        for l in range(1,self.num_layers):    \n",
    "            hidden[l] = self.recurrent_layers[l](torch.cat([hidden[l],outputs], 1))\n",
    "            outputs = self.dropout_layers[l](hidden[l])\n",
    "            if l == self.num_layers-1:\n",
    "                logits[seq] = self.linear_layers[l](outputs)\n",
    "            else:\n",
    "                outputs = self.linear_layers[l](outputs)\n",
    "    return logits.view(self.seq_len, self.batch_size, self.vocab_size), hidden\n",
    "\n",
    "  def generate(self, input, hidden, generated_seq_len):\n",
    "    # TODO ========================\n",
    "    # Compute the forward pass, as in the self.forward method (above).\n",
    "    # You'll probably want to copy substantial portions of that code here.\n",
    "    # \n",
    "    # We \"seed\" the generation by providing the first inputs.\n",
    "    # Subsequent inputs are generated by sampling from the output distribution, \n",
    "    # as described in the tex (Problem 5.3)\n",
    "    # Unlike for self.forward, you WILL need to apply the softmax activation \n",
    "    # function here in order to compute the parameters of the categorical \n",
    "    # distributions to be sampled from at each time-step.\n",
    "\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        - input: A mini-batch of input tokens (NOT sequences!)\n",
    "                        shape: (batch_size)\n",
    "        - hidden: The initial hidden states for every layer of the stacked RNN.\n",
    "                        shape: (num_layers, batch_size, hidden_size)\n",
    "        - generated_seq_len: The length of the sequence to generate.\n",
    "                       Note that this can be different than the length used \n",
    "                       for training (self.seq_len)\n",
    "    Returns:\n",
    "        - Sampled sequences of tokens\n",
    "                    shape: (generated_seq_len, batch_size)\n",
    "    \"\"\"\n",
    "    if inputs.is_cuda:\n",
    "        device = inputs.get_device()\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    \n",
    "    hidden = hidden.to(device)\n",
    "    \n",
    "    emb_inputs = self.embedding(inputs)\n",
    "    logits = torch.Tensor(self.seq_len, self.batch_size, self.vocab_size).to(device)\n",
    "    \n",
    "    for seq in range(self.seq_len):\n",
    "        \n",
    "        hidden[0] = self.recurrent_layers[0](torch.cat([hidden[0],emb_inputs[seq]], 1))\n",
    "        outputs = self.dropout_layers[0](hidden[0])\n",
    "        outputs = self.linear_layers[0](outputs)\n",
    "        if self.num_layers == 1:\n",
    "            logits[seq] = outputs\n",
    "        \n",
    "        for l in range(1,self.num_layers):    \n",
    "            hidden[l] = self.recurrent_layers[l](torch.cat([hidden[l],outputs], 1))\n",
    "            outputs = self.dropout_layers[l](hidden[l])\n",
    "            if l == self.num_layers-1:\n",
    "                logits[seq] = self.linear_layers[l](outputs)\n",
    "            else:\n",
    "                outputs = self.linear_layers[l](outputs)\n",
    "    return logits.view(self.seq_len, self.batch_size, self.vocab_size), hidden\n",
    "    \n",
    "    \n",
    "    samples = torch.Tensor(generated_seq_len, batch_size)\n",
    "    samples[0] = self.embedding(input) \n",
    "    for seq in range(generated_seq_len):\n",
    "        for l in range(self.num_layers):\n",
    "            hidden[l] = self.recurrent_layers[l](torch.cat([hidden[l],samples[seq]], 0))\n",
    "            if l < self.num_layers-1:\n",
    "                samples[seq] = self.dropout_layers[l](hidden[l])\n",
    "            samples[seq] = self.linear_layers[l](samples[seq])\n",
    "        if seq < generated_seq_len-1:\n",
    "            samples[seq+1] = samples[seq]\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runfile('C:/Users/Théo/Documents/PolyMtl/IFT6135_2019/02/ptb-lm.py', \n",
    "        wdir='C:/Users/Théo/Documents/PolyMtl/IFT6135_2019/02',\n",
    "        args = \"--model=RNN --optimizer=ADAM --initial_lr=0.0001 --batch_size=20 --seq_len=35 --hidden_size=1500 --num_layers=2 --dp_keep_prob=0.35 --save_best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########## Running Main Loop ##########################\n",
    "\n",
    "EPOCH 0 ------------------\n",
    "step: 10        loss: 3529.5392274856567        speed (wps):902.8174428959136\n",
    "step: 142       loss: 149279.704079628  speed (wps):905.5170478982327\n",
    "step: 274       loss: 179141.8760561943 speed (wps):905.3147800750617\n",
    "step: 406       loss: 208182.39816904068        speed (wps):905.3567834086997\n",
    "step: 538       loss: 237026.87060117722        speed (wps):906.2892257028814\n",
    "step: 670       loss: 265616.11352682114        speed (wps):882.6916074195929\n",
    "step: 802       loss: 293532.5080084801 speed (wps):860.783327800122\n",
    "step: 934       loss: 321377.4548268318 speed (wps):847.7529544398733\n",
    "step: 1066      loss: 349061.966881752  speed (wps):838.0461397122924\n",
    "step: 1198      loss: 376456.8313694    speed (wps):830.6268344897844\n",
    "Saving model parameters to best_params.pt\n",
    "epoch: 0        train ppl: 5858.483523467034    val ppl: 328.5533928651772      best val: 328.5533928651772     time (s) spent in epoch: 1144.3142385482788\n",
    "\n",
    "EPOCH 1 ------------------\n",
    "step: 10        loss: 2316.36301279068  speed (wps):766.8503092924894\n",
    "step: 142       loss: 29103.100430965424        speed (wps):776.1474559565766\n",
    "step: 274       loss: 56287.026228904724        speed (wps):775.6277608953311\n",
    "step: 406       loss: 83102.58867263794 speed (wps):775.2052193173345\n",
    "step: 538       loss: 110017.69404888153        speed (wps):773.8004343091643\n",
    "step: 670       loss: 136931.1484694481 speed (wps):766.0637896028011\n",
    "step: 802       loss: 163327.31778621674        speed (wps):760.6522099345661\n",
    "step: 934       loss: 189825.20168066025        speed (wps):761.5092705630851\n",
    "step: 1066      loss: 216283.2050061226 speed (wps):762.423939461976\n",
    "step: 1198      loss: 242481.0287284851 speed (wps):762.1537785624325\n",
    "Saving model parameters to best_params.pt\n",
    "epoch: 1        train ppl: 319.86563318348607   val ppl: 275.072699700709       best val: 275.072699700709      time (s) spent in epoch: 1238.5455334186554\n",
    "\n",
    "EPOCH 2 ------------------\n",
    "step: 10        loss: 2237.92653799057  speed (wps):747.3242029639088\n",
    "step: 142       loss: 28053.52163553238 speed (wps):761.9876604156635\n",
    "step: 274       loss: 54356.591074466705        speed (wps):783.9244763034252\n",
    "step: 406       loss: 80274.85654592514 speed (wps):812.5543017185942\n",
    "step: 538       loss: 106365.77887296677        speed (wps):833.2770952132511\n",
    "step: 670       loss: 132493.28870296478        speed (wps):845.4194815417692\n",
    "step: 802       loss: 158134.52977657318        speed (wps):854.5973764630872\n",
    "step: 934       loss: 183903.90338897705        speed (wps):846.3900259996085\n",
    "step: 1066      loss: 209641.54880523682        speed (wps):832.095814460482\n",
    "step: 1198      loss: 235115.7008910179 speed (wps):835.5292304773482\n",
    "Saving model parameters to best_params.pt\n",
    "epoch: 2        train ppl: 269.1902955017364    val ppl: 246.02598505865174     best val: 246.02598505865174    time (s) spent in epoch: 1120.8020842075348\n",
    "\n",
    "EPOCH 3 ------------------\n",
    "step: 10        loss: 2188.8030004501343        speed (wps):899.2219992800909\n",
    "step: 142       loss: 27348.993027210236        speed (wps):902.2241227184151\n",
    "step: 274       loss: 53097.99941778183 speed (wps):901.8645556226538\n",
    "step: 406       loss: 78409.74533319473 speed (wps):901.0715822155596\n",
    "step: 538       loss: 103925.39725065231        speed (wps):900.5218034094969\n",
    "step: 670       loss: 129458.16264390945        speed (wps):900.3086486357212\n",
    "step: 802       loss: 154512.99488544464        speed (wps):899.6741374322879\n",
    "step: 934       loss: 179771.21260643005        speed (wps):898.1826573111662\n",
    "step: 1066      loss: 204979.45505142212        speed (wps):897.9838274898742\n",
    "step: 1198      loss: 229894.02508735657        speed (wps):897.9317948499602\n",
    "Saving model parameters to best_params.pt\n",
    "epoch: 3        train ppl: 238.04715005435955   val ppl: 230.03872458201718     best val: 230.03872458201718    time (s) spent in epoch: 1051.5604801177979\n",
    "\n",
    "EPOCH 4 ------------------\n",
    "step: 10        loss: 2145.196261405945 speed (wps):893.750869470297\n",
    "step: 142       loss: 26805.494389533997        speed (wps):893.1453290268416\n",
    "step: 274       loss: 52106.375386714935        speed (wps):893.9751193702921\n",
    "step: 406       loss: 76931.94136619568 speed (wps):894.7747020426268\n",
    "step: 538       loss: 101979.91278648376        speed (wps):895.2599264182646\n",
    "step: 670       loss: 127064.57997322083        speed (wps):895.680142633723\n",
    "step: 802       loss: 151649.32209968567        speed (wps):896.3083975289065\n",
    "step: 934       loss: 176460.16721487045        speed (wps):896.1156240012864\n",
    "step: 1066      loss: 201272.98335313797        speed (wps):896.1272251852181\n",
    "step: 1198      loss: 225750.70413827896        speed (wps):896.4401033426635\n",
    "Saving model parameters to best_params.pt\n",
    "epoch: 4        train ppl: 215.87277186569682   val ppl: 221.251970920005       best val: 221.251970920005      time (s) spent in epoch: 1052.147334098816"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 2\n",
    "class GRU(nn.Module): # Implement a stacked GRU RNN\n",
    "  \"\"\"\n",
    "  Follow the same instructions as for RNN (above), but use the equations for \n",
    "  GRU, not Vanilla RNN.\n",
    "  \"\"\"\n",
    "  def __init__(self, emb_size, hidden_size, seq_len, batch_size, vocab_size, num_layers, dp_keep_prob):\n",
    "    super(GRU, self).__init__()\n",
    "\n",
    "    # TODO ========================\n",
    "\n",
    "  def init_weights_uniform(self):\n",
    "    # TODO ========================\n",
    "\n",
    "  def init_hidden(self):\n",
    "    # TODO ========================\n",
    "    return # a parameter tensor of shape (self.num_layers, self.batch_size, self.hidden_size)\n",
    "\n",
    "  def forward(self, inputs, hidden):\n",
    "    # TODO ========================\n",
    "    return logits.view(self.seq_len, self.batch_size, self.vocab_size), hidden\n",
    "\n",
    "  def generate(self, input, hidden, generated_seq_len):\n",
    "    # TODO ========================\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 3\n",
    "##############################################################################\n",
    "#\n",
    "# Code for the Transformer model\n",
    "#\n",
    "##############################################################################\n",
    "\n",
    "\"\"\"\n",
    "Implement the MultiHeadedAttention module of the transformer architecture.\n",
    "All other necessary modules have already been implemented for you.\n",
    "\n",
    "We're building a transfomer architecture for next-step prediction tasks, and \n",
    "applying it to sequential language modelling. We use a binary \"mask\" to specify \n",
    "which time-steps the model can use for the current prediction.\n",
    "This ensures that the model only attends to previous time-steps.\n",
    "\n",
    "The model first encodes inputs using the concatenation of a learned WordEmbedding \n",
    "and a (in our case, hard-coded) PositionalEncoding.\n",
    "The word embedding maps a word's one-hot encoding into a dense real vector.\n",
    "The positional encoding 'tags' each element of an input sequence with a code that \n",
    "identifies it's position (i.e. time-step).\n",
    "\n",
    "These encodings of the inputs are then transformed repeatedly using multiple\n",
    "copies of a TransformerBlock.\n",
    "This block consists of an application of MultiHeadedAttention, followed by a \n",
    "standard MLP; the MLP applies *the same* mapping at every position.\n",
    "Both the attention and the MLP are applied with Resnet-style skip connections, \n",
    "and layer normalization.\n",
    "\n",
    "The complete model consists of the embeddings, the stacked transformer blocks, \n",
    "and a linear layer followed by a softmax.\n",
    "\"\"\"\n",
    "\n",
    "#This code has been modified from an open-source project, by David Krueger.\n",
    "#The original license is included below:\n",
    "#MIT License\n",
    "#\n",
    "#Copyright (c) 2018 Alexander Rush\n",
    "#\n",
    "#Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "#of this software and associated documentation files (the \"Software\"), to deal\n",
    "#in the Software without restriction, including without limitation the rights\n",
    "#to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "#copies of the Software, and to permit persons to whom the Software is\n",
    "#furnished to do so, subject to the following conditions:\n",
    "#\n",
    "#The above copyright notice and this permission notice shall be included in all\n",
    "#copies or substantial portions of the Software.\n",
    "#\n",
    "#THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "#IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "#FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "#AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "#LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "#OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "#SOFTWARE.\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------\n",
    "\n",
    "# TODO: implement this class\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, n_heads, n_units, dropout=0.1):\n",
    "        \"\"\"\n",
    "        n_heads: the number of attention heads\n",
    "        n_units: the number of output units\n",
    "        dropout: probability of DROPPING units\n",
    "        \"\"\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        # This sets the size of the keys, values, and queries (self.d_k) to all \n",
    "        # be equal to the number of output units divided by the number of heads.\n",
    "        self.d_k = n_units // n_heads\n",
    "        # This requires the number of n_heads to evenly divide n_units.\n",
    "        assert n_units % n_heads == 0\n",
    "        self.n_units = n_units \n",
    "\n",
    "        # TODO: create/initialize any necessary parameters or layers\n",
    "        # Note: the only Pytorch modules you are allowed to use are nn.Linear \n",
    "        # and nn.Dropout\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # TODO: implement the masked multi-head attention.\n",
    "        # query, key, and value all have size: (batch_size, seq_len, self.n_units, self.d_k)\n",
    "        # mask has size: (batch_size, seq_len, seq_len)\n",
    "        # As described in the .tex, apply input masking to the softmax \n",
    "        # generating the \"attention values\" (i.e. A_i in the .tex)\n",
    "        # Also apply dropout to the attention values.\n",
    "\n",
    "        return # size: (batch_size, seq_len, self.n_units)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------\n",
    "# The encodings of elements of the input sequence\n",
    "\n",
    "class WordEmbedding(nn.Module):\n",
    "    def __init__(self, n_units, vocab):\n",
    "        super(WordEmbedding, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, n_units)\n",
    "        self.n_units = n_units\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print (x)\n",
    "        return self.lut(x) * math.sqrt(self.n_units)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, n_units, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, n_units)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, n_units, 2).float() *\n",
    "                             -(math.log(10000.0) / n_units))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------\n",
    "# The TransformerBlock and the full Transformer\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(ResidualSkipConnectionWithLayerNorm(size, dropout), 2)\n",
    " \n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask)) # apply the self-attention\n",
    "        return self.sublayer[1](x, self.feed_forward) # apply the position-wise MLP\n",
    "\n",
    "\n",
    "class TransformerStack(nn.Module):\n",
    "    \"\"\"\n",
    "    This will be called on the TransformerBlock (above) to create a stack.\n",
    "    \"\"\"\n",
    "    def __init__(self, layer, n_blocks): # layer will be TransformerBlock (below)\n",
    "        super(TransformerStack, self).__init__()\n",
    "        self.layers = clones(layer, n_blocks)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "class FullTransformer(nn.Module):\n",
    "    def __init__(self, transformer_stack, embedding, n_units, vocab_size):\n",
    "        super(FullTransformer, self).__init__()\n",
    "        self.transformer_stack = transformer_stack\n",
    "        self.embedding = embedding\n",
    "        self.output_layer = nn.Linear(n_units, vocab_size)\n",
    "        \n",
    "    def forward(self, input_sequence, mask):\n",
    "        embeddings = self.embedding(input_sequence)\n",
    "        return F.log_softmax(self.output_layer(self.transformer_stack(embeddings, mask)), dim=-1)\n",
    "\n",
    "\n",
    "def make_model(vocab_size, n_blocks=6, \n",
    "               n_units=512, n_heads=16, dropout=0.1):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(n_heads, n_units)\n",
    "    ff = MLP(n_units, dropout)\n",
    "    position = PositionalEncoding(n_units, dropout)\n",
    "    model = FullTransformer(\n",
    "        transformer_stack=TransformerStack(TransformerBlock(n_units, c(attn), c(ff), dropout), n_blocks),\n",
    "        embedding=nn.Sequential(WordEmbedding(n_units, vocab_size), c(position)),\n",
    "        n_units=n_units,\n",
    "        vocab_size=vocab_size\n",
    "        )\n",
    "    \n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------\n",
    "# Data processing\n",
    "\n",
    "def subsequent_mask(size):\n",
    "    \"\"\" helper function for creating the masks. \"\"\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0\n",
    "\n",
    "class Batch:\n",
    "    \"Object for holding a batch of data with mask during training.\"\n",
    "    def __init__(self, x, pad=0):\n",
    "        self.data = x\n",
    "        self.mask = self.make_mask(self.data, pad)\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_mask(data, pad):\n",
    "        \"Create a mask to hide future words.\"\n",
    "        mask = (data != pad).unsqueeze(-2)\n",
    "        mask = mask & Variable(\n",
    "            subsequent_mask(data.size(-1)).type_as(mask.data))\n",
    "        return mask\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------\n",
    "# Some standard modules\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"layer normalization, as in: https://arxiv.org/abs/1607.06450\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "\n",
    "class ResidualSkipConnectionWithLayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(ResidualSkipConnectionWithLayerNorm, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    This is just an MLP with 1 hidden layer\n",
    "    \"\"\"\n",
    "    def __init__(self, n_units, dropout=0.1):\n",
    "        super(MLP, self).__init__()\n",
    "        self.w_1 = nn.Linear(n_units, 2048)\n",
    "        self.w_2 = nn.Linear(2048, n_units)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
