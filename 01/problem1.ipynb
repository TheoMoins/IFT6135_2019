{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.load(\"data/mnist.npz\")\n",
    "x_train, y_train = data[\"x_train\"], data[\"y_train\"]\n",
    "x_test, y_test = data[\"x_test\"], data[\"y_test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape to vectors, change dtype from int8 to float32 and normalize [0,255] -> [0,1]\n",
    "x_train = x_train.reshape(x_train.shape[0], -1).astype(np.float32) / 255\n",
    "x_test = x_test.reshape(x_test.shape[0], -1).astype(np.float32) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a validation set\n",
    "from sklearn import model_selection\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = model_selection.train_test_split(\n",
    "    x_train, y_train, test_size=10000, stratify=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "class Classifier(object):\n",
    "    def __init__(self,\n",
    "                 hidden_dims=(200, 100),\n",
    "                 n_hidden=2,\n",
    "                 mode=\"train\",\n",
    "                 datapath=None,\n",
    "                 model_path=None):\n",
    "        #weights and bias structure\n",
    "        self.weights, self.bias = [], []\n",
    "        #layer 1\n",
    "        self.weights.append(np.empty((hidden_dims[0], 784)))\n",
    "        self.bias.append(np.zeros((hidden_dims[0])))\n",
    "        #hidden layer\n",
    "        for i in range(n_hidden - 1):\n",
    "            self.weights.append(np.empty((hidden_dims[i + 1], hidden_dims[i])))\n",
    "            self.bias.append(np.zeros((hidden_dims[i + 1])))\n",
    "        #output layer\n",
    "        self.weights.append(np.empty((10, hidden_dims[-1])))\n",
    "        self.bias.append(np.zeros((10)))\n",
    "\n",
    "    def initialize_weights(self, method=\"glorot\"):\n",
    "        for i, w in enumerate(self.weights):\n",
    "            if method is \"glorot\":\n",
    "                d = math.sqrt(6 / (w.shape[0] + w.shape[1]))\n",
    "                self.weights[i] = np.random.uniform(\n",
    "                    low=-d, high=d, size=w.shape)\n",
    "            if method is \"normal\":\n",
    "                self.weights[i] = np.random.normal(\n",
    "                    loc=0, scale=1, size=w.shape)\n",
    "            if method is \"zero\":\n",
    "                self.weights[i] = np.zeros(shape=w.shape)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.cache = [input]\n",
    "        for w, b in zip(self.weights, self.bias):\n",
    "            self.cache.append(self.activation(w @ self.cache[-1] + b))\n",
    "        return self.softmax(self.cache.pop())\n",
    "\n",
    "    def activation(self, input):\n",
    "        return np.maximum(0, input)\n",
    "\n",
    "    def loss(self, prediction, label):\n",
    "        return -math.log(self.softmax(prediction)[label])\n",
    "\n",
    "    def softmax(self, input):\n",
    "        m = np.max(input)\n",
    "        return np.exp(input - m) / np.sum(np.exp(input - m))\n",
    "\n",
    "    def backward(self, output, label):\n",
    "        grad_pre_activation = np.asarray([\n",
    "            out - 1 if i == label else out for i, out in enumerate(output)\n",
    "        ]).reshape(-1, 1)\n",
    "        self.grad_w, self.grad_b = [], []\n",
    "        # we go from the last layer to the first one\n",
    "        for i, (w, b) in enumerate(\n",
    "                zip(reversed(self.weights), reversed(self.bias))):\n",
    "            previous_hidden_layer = np.asarray(list(reversed(self.cache))[i])\n",
    "            self.grad_w.insert(\n",
    "                0,\n",
    "                grad_pre_activation @ previous_hidden_layer.reshape(-1, 1).T)\n",
    "            self.grad_b.insert(0, grad_pre_activation.reshape(-1))\n",
    "            grad_previous_hidden_layer = w.T @ grad_pre_activation\n",
    "            grad_pre_activation = grad_previous_hidden_layer * np.asarray(\n",
    "                [1 if x > 0 else 0\n",
    "                 for x in list(reversed(self.cache))[i]]).reshape(-1, 1)\n",
    "\n",
    "    def update(self, lr):\n",
    "        for i, (gw, gb) in enumerate(zip(self.grad_w, self.grad_b)):\n",
    "            self.weights[i] = self.weights[i] - lr * gw\n",
    "            self.bias[i] = self.bias[i] - lr * gb\n",
    "\n",
    "    def train(self, inputs, labels, epochs=1, lr=0.001, verbose=True):\n",
    "        total_loss = []\n",
    "        for epoch in range(epochs):\n",
    "            loss = []\n",
    "            data = list(zip(inputs, labels))\n",
    "            random.shuffle(data)\n",
    "            inputs, labels = zip(*data)\n",
    "            for i, (x, y) in enumerate(zip(inputs, labels), 1):\n",
    "                pred = clf.forward(x)\n",
    "                loss.append(clf.loss(pred, y))\n",
    "                if verbose and i % 100 == 0:\n",
    "                    print(\n",
    "                        \"\\repoch {:2d}: {:.3f}\".format(epoch + 1,\n",
    "                                                      np.mean(loss)),\n",
    "                        end=\"\")\n",
    "                clf.backward(pred, y)\n",
    "                clf.update(lr)\n",
    "            if verbose:\n",
    "                print(\"\\repoch {:2d}: {:.3f}\".format(epoch + 1, np.mean(loss)))\n",
    "            total_loss.append(np.mean(loss))\n",
    "        return total_loss\n",
    "\n",
    "    def test(self, inputs, labels):\n",
    "        loss, acc = zip(*[(self.loss(self.forward(x), y),\n",
    "                           np.argmax(self.forward(x)) == y)\n",
    "                          for x, y in zip(inputs, labels)])\n",
    "        loss, acc = np.mean(loss), np.mean(acc)\n",
    "        return np.mean(loss), np.mean(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1: 2.302"
     ]
    }
   ],
   "source": [
    "clf = Classifier()\n",
    "clf.initialize_weights(\"zero\")\n",
    "z_loss = clf.train(x_train, y_train, epochs=10, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Classifier()\n",
    "clf.initialize_weights(\"normal\")\n",
    "n_loss = clf.train(x_train, y_train, epochs=10, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Classifier()\n",
    "clf.initialize_weights(\"glorot\")\n",
    "g_loss = clf.train(x_train, y_train, epochs=10, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(range(0, 10), z_loss, label=\"zero\")\n",
    "plt.plot(range(0, 10), n_loss, label=\"normal\")\n",
    "plt.plot(range(0, 10), g_loss, label=\"glorot\")\n",
    "plt.legend(title=\"W initialization\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:^10s} | {:^10s} | {:^10s} | {:^10s}\".format(\"hidden 1\", \"hidden 2\",\n",
    "                                                     \"lr\", \"accuracy\"))\n",
    "print(49 * \"-\")\n",
    "best_model = None\n",
    "best_acc = -1\n",
    "for h1 in [400, 200, 100]:\n",
    "    for h2 in [200, 100, 50]:\n",
    "        for l in [1e-1, 1e-2, 1e-3]:\n",
    "            clf = Classifier(hidden_dims=(h1, h2))\n",
    "            clf.initialize_weights(\"glorot\")\n",
    "            clf.train(x_train, y_train, epochs=10, lr=l, verbose=False)\n",
    "            loss, acc = clf.test(x_valid, y_valid)\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_model = clf\n",
    "            print(\"{:^10d} | {:^10d} | {:^10g} | {:^10.1%}\".format(\n",
    "                h1, h2, l, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy of the model on the 10000 test images: {:3.1%}\".format(clf.test(x_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Gradients using Finite Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Classifier(hidden_dims=(100, 50))\n",
    "clf.initialize_weights(\"glorot\")\n",
    "clf.train(x_train, y_train, epochs=10)\n",
    "x, y = x_train[0], y_train[0]\n",
    "clf.backward(clf.forward(x), y)\n",
    "mean_abs_delta, max_abs_delta = [], []\n",
    "N_values = [i * 10**exp for exp in range(0, 5) for i in range(1, 10)]\n",
    "for N in N_values:\n",
    "    delta = []\n",
    "    for i, (W, gW) in enumerate(zip(clf.weights[0], clf.grad_w[0])):\n",
    "        if i > 9:\n",
    "            break\n",
    "        for j, (w, gw) in enumerate(zip(W, gW)):\n",
    "            clf.weights[0][i][j] = w + 1 / N\n",
    "            loss1 = clf.loss(clf.forward(x), y)\n",
    "            clf.weights[0][i][j] = w - 1 / N\n",
    "            loss2 = clf.loss(clf.forward(x), y)\n",
    "            estimate_grad = (loss1 - loss2) / (2 / N)\n",
    "            delta.append(estimate_grad - gw)\n",
    "    max_abs_delta.append(np.max(np.absolute(delta)))\n",
    "    mean_abs_delta.append(np.mean(np.absolute(delta)))\n",
    "\n",
    "plt.semilogx(N_values, max_abs_delta, label=\"max absolute delta\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.semilogx(N_values, mean_abs_delta, label=\"mean absolute delta\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
