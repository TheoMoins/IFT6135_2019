{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.load('mnist.npz')\n",
    "x_train, y_train = data['x_train'], data['y_train']\n",
    "x_test, y_test = data['x_test'], data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape to vectors, change dtype from int8 to float32 and normalize [0,255] -> [0,1]\n",
    "x_train = x_train.reshape(x_train.shape[0], -1).astype(np.float32) / 255\n",
    "x_test = x_test.reshape(x_test.shape[0], -1).astype(np.float32) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a validation set\n",
    "from sklearn import model_selection\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = model_selection.train_test_split(\n",
    "    x_train, y_train, test_size=10000, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class Classifier(object):\n",
    "    def __init__(self,\n",
    "                 hidden_dims=(400, 100),\n",
    "                 n_hidden=2,\n",
    "                 mode=\"train\",\n",
    "                 datapath=None,\n",
    "                 model_path=None):\n",
    "        #weights and bias structure\n",
    "        self.weights, self.bias = [], []\n",
    "        #layer 1\n",
    "        self.weights.append(np.empty((hidden_dims[0], 784)))\n",
    "        self.bias.append(np.zeros((hidden_dims[0])))\n",
    "        #hidden layer\n",
    "        for i in range(n_hidden - 1):\n",
    "            self.weights.append(np.empty((hidden_dims[i + 1], hidden_dims[i])))\n",
    "            self.bias.append(np.zeros((hidden_dims[i + 1])))\n",
    "        #output layer\n",
    "        self.weights.append(np.empty((10, hidden_dims[-1])))\n",
    "        self.bias.append(np.zeros((10)))\n",
    "\n",
    "    def initialize_weights(self, method=\"glorot\"):\n",
    "        for i, w in enumerate(self.weights):\n",
    "            if method is \"glorot\":\n",
    "                d = math.sqrt(6 / (w.shape[0] + w.shape[1]))\n",
    "                self.weights[i] = np.random.uniform(\n",
    "                    low=-d, high=d, size=w.shape)\n",
    "            if method is \"normal\":\n",
    "                self.weights[i] = np.random.normal(\n",
    "                    loc=0, scale=1, size=w.shape)\n",
    "            if method is \"zero\":\n",
    "                self.weights[i] = np.zeros(shape=w.shape)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.cache = [input]\n",
    "        for w, b in zip(self.weights, self.bias):\n",
    "            self.cache.append(self.activation(w @ self.cache[-1] + b))\n",
    "        return self.softmax(self.cache.pop())\n",
    "\n",
    "    def activation(self, input):\n",
    "        return np.maximum(0, input)\n",
    "\n",
    "    def loss(self, prediction, label):\n",
    "        return -math.log(self.softmax(prediction)[label])\n",
    "\n",
    "    def softmax(self, input):\n",
    "        m = np.max(input)\n",
    "        return np.exp(input - m) / np.sum(np.exp(input - m))\n",
    "\n",
    "    def backward(self, output, label):\n",
    "        grad_pre_activation = np.asarray([\n",
    "            out - 1 if i == label else out for i, out in enumerate(output)\n",
    "        ]).reshape(-1, 1)\n",
    "        self.grad_w, self.grad_b = [], []\n",
    "        # we go from the last layer to the first one\n",
    "        for i, (w, b) in enumerate(\n",
    "                zip(reversed(self.weights), reversed(self.bias))):\n",
    "            previous_hidden_layer = np.asarray(list(reversed(self.cache))[i])\n",
    "            self.grad_w.insert(\n",
    "                0,\n",
    "                grad_pre_activation @ previous_hidden_layer.reshape(-1, 1).T)\n",
    "            self.grad_b.insert(0, grad_pre_activation.reshape(-1))\n",
    "            grad_previous_hidden_layer = w.T @ grad_pre_activation\n",
    "            grad_pre_activation = grad_previous_hidden_layer * np.asarray(\n",
    "                [1 if x > 0 else 0\n",
    "                 for x in list(reversed(self.cache))[i]]).reshape(-1, 1)\n",
    "\n",
    "    def update(self, lr):\n",
    "        for i, (gw, gb) in enumerate(zip(self.grad_w, self.grad_b)):\n",
    "            self.weights[i] = self.weights[i] - lr * gw\n",
    "            self.bias[i] = self.bias[i] - lr * gb\n",
    "\n",
    "    def train(self, inputs, labels, epochs=1, lr=0.001, verbose=True):\n",
    "        total_loss = []\n",
    "        for epoch in range(epochs):\n",
    "            loss = []\n",
    "            for i, (x, y) in enumerate(zip(inputs, labels), 1):\n",
    "                pred = clf.forward(x)\n",
    "                loss.append(clf.loss(pred, y))\n",
    "                if verbose and i % 100 == 0:\n",
    "                    print(\n",
    "                        \"\\repoch {:2d}: {:.3f}\".format(epoch + 1, np.mean(loss)),\n",
    "                        end=\"\")\n",
    "                clf.backward(pred, y)\n",
    "                clf.update(lr)\n",
    "            if verbose:\n",
    "                print(\"\\repoch {:2d}: {:.3f}\".format(epoch + 1, np.mean(loss)))\n",
    "            total_loss.append(np.mean(loss))\n",
    "        return total_loss\n",
    "\n",
    "    def test(self, inputs, labels):\n",
    "        loss, acc = zip(*[(self.loss(self.forward(x), y),\n",
    "                           np.argmax(self.forward(x)) == y)\n",
    "                          for x, y in zip(inputs, labels)])\n",
    "        loss, acc = np.mean(loss), np.mean(acc)\n",
    "        return np.mean(loss), np.mean(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1: 2.302\n",
      "epoch  2: 2.302\n",
      "epoch  3: 2.302\n",
      "epoch  4: 2.302\n",
      "epoch  5: 2.302\n",
      "epoch  6: 2.302\n",
      "epoch  7: 2.302\n",
      "epoch  8: 2.302\n",
      "epoch  9: 2.302\n",
      "epoch 10: 2.302\n"
     ]
    }
   ],
   "source": [
    "clf = Classifier()\n",
    "clf.initialize_weights(\"zero\")\n",
    "z_loss = clf.train(x_train, y_train, epochs=10, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1: 1.926\n",
      "epoch  2: 1.797\n",
      "epoch  3: 1.727\n",
      "epoch  4: 1.686\n",
      "epoch  5: 1.662\n",
      "epoch  6: 1.647\n",
      "epoch  7: 1.635\n",
      "epoch  8: 1.625\n",
      "epoch  9: 1.619\n",
      "epoch 10: 1.611\n"
     ]
    }
   ],
   "source": [
    "clf = Classifier()\n",
    "clf.initialize_weights(\"normal\")\n",
    "n_loss = clf.train(x_train, y_train, epochs=10, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1: 1.645\n",
      "epoch  2: 1.551\n",
      "epoch  3: 1.527\n",
      "epoch  4: 1.514\n",
      "epoch  5: 1.504\n",
      "epoch  6: 1.497\n",
      "epoch  7: 1.492\n",
      "epoch  8: 1.487\n",
      "epoch  9: 1.484\n",
      "epoch 10: 1.481\n"
     ]
    }
   ],
   "source": [
    "clf = Classifier()\n",
    "clf.initialize_weights(\"glorot\")\n",
    "g_loss = clf.train(x_train, y_train, epochs=10, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(range(0, 10), z_loss, label=\"zero\")\n",
    "plt.plot(range(0, 10), n_loss, label=\"normal\")\n",
    "plt.plot(range(0, 10), g_loss, label=\"glorot\")\n",
    "plt.legend(title=\"W initialization\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hidden 1  |  hidden 2  |     lr     |  accuracy \n",
      "-------------------------------------------------\n",
      "   400     |    200     |    0.1     |    9.9%   \n",
      "   400     |    200     |    0.01    |   98.0%   \n",
      "   400     |    200     |   0.001    |   97.6%   \n",
      "   400     |    100     |    0.1     |    9.9%   \n",
      "   400     |    100     |    0.01    |   97.6%   \n",
      "   400     |    100     |   0.001    |   97.6%   \n",
      "   400     |     50     |    0.1     |    9.9%   \n",
      "   400     |     50     |    0.01    |   97.8%   \n",
      "   400     |     50     |   0.001    |   97.5%   \n",
      "   200     |    200     |    0.1     |    9.9%   \n",
      "   200     |    200     |    0.01    |   97.5%   \n",
      "   200     |    200     |   0.001    |   97.2%   \n",
      "   200     |    100     |    0.1     |    9.9%   \n",
      "   200     |    100     |    0.01    |   97.8%   \n",
      "   200     |    100     |   0.001    |   97.3%   \n",
      "   200     |     50     |    0.1     |    9.9%   \n",
      "   200     |     50     |    0.01    |   97.6%   \n",
      "   200     |     50     |   0.001    |   97.5%   \n",
      "   100     |    200     |    0.1     |    9.9%   \n",
      "   100     |    200     |    0.01    |   97.2%   \n",
      "   100     |    200     |   0.001    |   97.2%   \n",
      "   100     |    100     |    0.1     |    9.9%   \n",
      "   100     |    100     |    0.01    |   97.2%   \n",
      "   100     |    100     |   0.001    |   97.2%   \n",
      "   100     |     50     |    0.1     |    9.9%   \n",
      "   100     |     50     |    0.01    |   97.1%   \n",
      "   100     |     50     |   0.001    |   97.2%   \n"
     ]
    }
   ],
   "source": [
    "print(\"{:^10s} | {:^10s} | {:^10s} | {:^10s}\".format(\"hidden 1\", \"hidden 2\", \"lr\", \"accuracy\"))\n",
    "print(49 * \"-\")\n",
    "for h1 in [400, 200, 100]:\n",
    "    for h2 in [200, 100, 50]:\n",
    "        for l in [1e-1, 1e-2, 1e-3]:\n",
    "            clf = Classifier(hidden_dims=(h1, h2))\n",
    "            clf.initialize_weights('glorot')\n",
    "            clf.train(x_train, y_train, epochs=10, lr=l, verbose=False)\n",
    "            loss, acc = clf.test(x_valid, y_valid)\n",
    "            print(\"{:^10d} | {:^10d} | {:^10g} | {:^10.1%}\".format(h1, h2, l, acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
